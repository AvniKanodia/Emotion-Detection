{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download data and setup environment"
      ],
      "metadata": {
        "id": "WsYzL658GfV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import gdown\n",
        "import pickle\n",
        "import warnings\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "from scipy.spatial import distance\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "###Getting the csv data loaded\n",
        "!wget -q --show-progress -O ./ferdata.csv \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/fer2013_5.csv\"\n",
        "\n",
        "###Getting the Dlib Shape predictor!\n",
        "!wget -q --show-progress -O ./shape_predictor_68_face_landmarks.dat \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "###Getting the Xpure loaded\n",
        "!wget -q --show-progress -O ./pureX.npy \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/pureX.npy\"\n",
        "\n",
        "###Getting the Xdata loaded\n",
        "!wget -q --show-progress -O ./dataX.npy \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/dataX_edited.npy\"\n",
        "\n",
        "###Getting the Ydata loaded\n",
        "!wget -q --show-progress -O ./dataY.npy \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/dataY.npy\"\n",
        "\n",
        "print (\"Data Downloaded!\")"
      ],
      "metadata": {
        "id": "dOVfk4slGi5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset"
      ],
      "metadata": {
        "id": "A-j1OcTZGnck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Integer to Label Mapping\n",
        "label_map = {0:\"ANGRY\",1:\"HAPPY\",2:\"SAD\",3:\"SURPRISE\",4:\"NEUTRAL\"}\n",
        "\n",
        "#Load the data\n",
        "df = pd.read_csv(\"./ferdata.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "pqEGpxcrGqh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate x labels for our plot\n",
        "emotion_labels = [label_map[i] for i in label_map.keys()]\n",
        "\n",
        "# generate counts for each emotion type\n",
        "emotion_counts = [np.sum(df[\"emotion\"] == i) for i in range(len(label_map))]\n",
        "\n",
        "# generate a bar plot for our emotion labels that has different colors \n",
        "[plt.bar(x = emotion_labels[i], height = emotion_counts[i] ) for i in range(len(emotion_labels))] \n",
        "\n",
        "# make the plot interpretable with x and y labels + title\n",
        "plt.xlabel('EMOTION LABEL')\n",
        "plt.ylabel('N OBSERVSATIONS')\n",
        "plt.title('A balanced distribution of emotions in our data set', y=1.05); "
      ],
      "metadata": {
        "id": "o5U7DcWQGv03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraction of Facial Landmarks"
      ],
      "metadata": {
        "id": "rIsxLyNiHFOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')\n",
        "\"\"\"\n",
        "Returns facial landmarks for the given input image path\n",
        "\"\"\"\n",
        "def get_landmarks(image):\n",
        "  \n",
        "  \n",
        "  #:type image : cv2 object\n",
        "  #:rtype landmarks : list of tuples where each tuple represents \n",
        "  #                  the x and y co-ordinates of facial keypoints\n",
        "  \n",
        "  #Bounding Box co-ordinates around the face(Training data is 48*48(cropped faces))\n",
        "  rects = [dlib.rectangle(left=1, top=1, right=47, bottom=47)]\n",
        "\n",
        "  #Read Image using OpenCV\n",
        "  #image = cv2.imread(image_path)\n",
        "  #Detect the Faces within the image\n",
        "  landmarks = [(p.x, p.y) for p in predictor(image, rects[0]).parts()]\n",
        "  return image,landmarks\n",
        "\n",
        "\"\"\"\n",
        "Display image with its Facial Landmarks\n",
        "\"\"\"\n",
        "def plot_image_landmarks(image,face_landmarks):\n",
        "  \"\"\"\n",
        "  :type image_path : str\n",
        "  :type face_landmarks : list of tuples where each tuple represents \n",
        "                     the x and y co-ordinates of facial keypoints\n",
        "  :rtype : None\n",
        "  \"\"\"\n",
        "  radius = -2\n",
        "  circle_thickness = 1\n",
        "  image_copy = image.copy()\n",
        "  for (x, y) in face_landmarks:\n",
        "    cv2.circle(image_copy, (x, y), circle_thickness, (255,0,0), radius)\n",
        "    \n",
        "  plt.imshow(image_copy, interpolation='nearest', cmap='Greys_r')\n",
        "  plt.xticks([]); plt.yticks([])\n",
        "  plt.show()\n",
        "  \n",
        " \n",
        "'''\n",
        "Converts pixels values to 2D-image. \n",
        "Displays the image and returns the cv2 image\n",
        "object\n",
        "'''\n",
        "def get_pixels_image(img_pixels,plt_flag):\n",
        "  \"\"\"\n",
        "  :type image_pixels : str\n",
        "  :type plt_flag : boolean\n",
        "  :rtype image : cv2 object\n",
        "  \"\"\"\n",
        "  \n",
        "  width = 48\n",
        "  height = 48\n",
        "  \n",
        "  image = np.fromstring(img_pixels, dtype=np.uint8, sep=\" \").reshape((height, width))\n",
        "  \n",
        "  if plt_flag:\n",
        "      plt.imshow(image, interpolation='nearest', cmap=\"Greys_r\")\n",
        "      plt.xticks([]); plt.yticks([])\n",
        "      plt.show()\n",
        "      \n",
        "      \n",
        "  return image"
      ],
      "metadata": {
        "id": "UyCYuKfVG6xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Data Points"
      ],
      "metadata": {
        "id": "5-vlVDQUHLpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select random index \n",
        "i_index = np.random.randint(len(df))\n",
        "\n",
        "# extract pixel values \n",
        "image_pixels = df['pixels'][i_index]        \n",
        "\n",
        "# convert pixels to 2D Images\n",
        "image = get_pixels_image(image_pixels, True)"
      ],
      "metadata": {
        "id": "Plw8zlRFHQsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Facial Landmarks"
      ],
      "metadata": {
        "id": "t39WyJS5HUpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the Facial Landmarks\n",
        "image,facial_landmarks = get_landmarks(image)\n",
        "\n",
        "#Display the Facial Landmarks on the Image\n",
        "plot_image_landmarks(image,facial_landmarks)"
      ],
      "metadata": {
        "id": "pBlo8Uh3HXEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distance between Facial Landmarks"
      ],
      "metadata": {
        "id": "LnNXTEx8HlRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_landmarks_euclid_dist(face_landmarks):\n",
        "  \n",
        "    e_dist = []\n",
        "    for i, j in itertools.combinations(range(68), 2):\n",
        "      e_dist.append(distance.euclidean(face_landmarks[i], face_landmarks[j]))\n",
        "    \n",
        "    return e_dist"
      ],
      "metadata": {
        "id": "smY_x-9-HoPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess data to extract distance between all points"
      ],
      "metadata": {
        "id": "0JhAlg5NHrZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "  \n",
        "  X = []\n",
        "  Y = []\n",
        "  X_pixels = []\n",
        "  \n",
        "  n_pixels = 2304\n",
        "  \n",
        "  for index, row in (df.iterrows()):\n",
        "\n",
        "      if index%1000 == 0:\n",
        "        print (index, \"Datapoints Processed\")\n",
        "        \n",
        "      try:\n",
        "          image = get_pixels_image(row['pixels'],0)\n",
        "          X_pixels.append(image.ravel()) \n",
        "          image = cv2.GaussianBlur(image,(5,5),0)\n",
        "         \n",
        "          _,face_landmarks = get_landmarks(image)\n",
        "          X.append(get_all_landmarks_euclid_dist(face_landmarks)) # Using our feature function!\n",
        "          Y.append(row['emotion'])\n",
        "\n",
        "      except Exception as e:\n",
        "          print (\"An error occured:\",e)\n",
        "\n",
        "  #Save the data \n",
        "  np.save(\"pureX\", X_pixels)\n",
        "  np.save(\"dataX\", X)\n",
        "  np.save(\"dataY\", Y)\n",
        "  \n",
        "  return np.array(X_pixels),np.array(X),np.array(Y) "
      ],
      "metadata": {
        "id": "hSyCc0QZHvAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Saved Data"
      ],
      "metadata": {
        "id": "-hZ7kratHzat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preload = True \n",
        "\n",
        "if preload: \n",
        "\n",
        "  # load outputs saved in this folder after running preprocess_data() \n",
        "  dataX = np.load('./dataX.npy')\n",
        "  dataY = np.load('./dataY.npy', allow_pickle=True)\n",
        "  \n",
        "else: \n",
        "  \n",
        "  # this takes 15-20 minutes to run, but someone has already run it and saved the ouputs in this folder\n",
        "  pureX, dataX, dataY = preprocess_data(df)"
      ],
      "metadata": {
        "id": "nOJyt7cNH0s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the Data"
      ],
      "metadata": {
        "id": "9IiYG_-nH78g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split Data into Train, Test (90-10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=0.1, random_state=42,stratify =dataY)"
      ],
      "metadata": {
        "id": "tFIgL0sEH9ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train,y_train)\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "id": "BTI_peZWIAIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardize the data"
      ],
      "metadata": {
        "id": "dS4PnUn-IRVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "m1-0LYq_IS3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "iGtvLXmcIVUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality Reduction"
      ],
      "metadata": {
        "id": "pif0aBt3IbWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(.95)\n",
        "pca.fit(X_train)\n",
        "\n",
        "X_train = pca.transform(X_train)\n",
        "X_test= pca.transform(X_test)"
      ],
      "metadata": {
        "id": "x5yq0v7AIdQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "s18RG6WrIgAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate model"
      ],
      "metadata": {
        "id": "uQdoSiwbIsLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######Train the model##################\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "print (\"Training the knn model\")\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "#######Evaluate the model##################\n",
        "#This cell will take longer to run(5-10mins)\n",
        "print (\"Predict for KNN Model\")\n",
        "y_predknn = knn.predict(X_test)\n",
        "print (\"Prediction Completed\")\n",
        "print (\"Test Accuracy(KNN):\",metrics.accuracy_score(y_test, y_predknn))\n",
        "\n",
        "#-----------------DecisionTreeClassifier--------------#\n",
        "#######Train the model##################\n",
        "dt = DecisionTreeClassifier(max_depth=20)\n",
        "print (\"Training the Decision Tree model\")\n",
        "dt.fit(X_train, y_train)\n",
        "print (\"Completed Decision Tree Training\")\n",
        "\n",
        "#######Evaluate the model##################\n",
        "\n",
        "print (\"Predict for Decision Tree Model\")\n",
        "y_preddt = dt.predict(X_test)\n",
        "print (\"Test Accuracy(DT):\",metrics.accuracy_score(y_test, y_preddt))\n",
        "\n",
        "#-----------------Logistic Regression--------------#\n",
        "#######Train the model##################\n",
        "lr = LogisticRegression(solver='lbfgs',multi_class='multinomial')\n",
        "print (\"Training the Logistic Regression model\")\n",
        "lr.fit(X_train, y_train)\n",
        "print (\"Completed LR Training\")\n",
        "\n",
        "#######Evaluate the model##################\n",
        "#This cell will take longer to run(5-10mins)\n",
        "print (\"Predict for LR Model\")\n",
        "y_predlr = lr.predict(X_test)\n",
        "print (\"Test Accuracy(LR):\",metrics.accuracy_score(y_test, y_predlr))"
      ],
      "metadata": {
        "id": "o1dnoNXNI5F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the confusion matrix"
      ],
      "metadata": {
        "id": "wy7vrpFhJG3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true,y_predicted):\n",
        "  cm = metrics.confusion_matrix(y_true, y_predicted)\n",
        "  print (\"Plotting the Confusion Matrix\")\n",
        "  labels = list(label_map.values())\n",
        "  df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "  fig = plt.figure()\n",
        "  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "  plt.yticks([0.5,1.5,2.5,3.5,4.5], labels,va='center')\n",
        "  plt.title('Confusion Matrix - TestData')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "oz8jwQXTJMWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test,y_predlr)"
      ],
      "metadata": {
        "id": "KyoZvfKAJOAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traning Model"
      ],
      "metadata": {
        "id": "iYFqQFGEJVTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.load('pureX.npy')\n",
        "Y = np.load('dataY.npy')"
      ],
      "metadata": {
        "id": "4SHCBKWJJWbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split Data into Train, Test (90-10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42,stratify =Y)\n",
        "\n",
        "\n",
        "#Standardize the Data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#PCA #Returns 264 features out of 2304\n",
        "pca = PCA(.95)\n",
        "pca.fit(X_train)\n",
        "\n",
        "X_train = pca.transform(X_train)\n",
        "X_test= pca.transform(X_test)\n",
        "\n",
        "\n",
        "# student code may look something like...\n",
        "\n",
        "#######Train the model##################\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "print (\"Training the knn model\")\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "#######Evaluate the model##################\n",
        "# they might use accuracy_score\n",
        "#This cell will take longer to run(5-10mins)!\n",
        "print (\"Predict for KNN Model\")\n",
        "y_predknn = knn.predict(X_test)\n",
        "print (\"Prediction Completed\")\n",
        "print (\"Test Accuracy(KNN):\",metrics.accuracy_score(y_test, y_predknn))\n",
        "\n",
        "\n",
        "\n",
        "#-----------------DecisionTreeClassifier--------------#\n",
        "#######Train the model##################\n",
        "dt = DecisionTreeClassifier(max_depth=20)\n",
        "print (\"Training the Decision Tree model\")\n",
        "dt.fit(X_train, y_train)\n",
        "print (\"Completed Decision Tree Training\")\n",
        "\n",
        "#######Evaluate the model##################\n",
        "\n",
        "print (\"Predict for Decision Tree Model\")\n",
        "y_preddt = dt.predict(X_test)\n",
        "print (\"Test Accuracy(DT):\",metrics.accuracy_score(y_test, y_preddt))\n",
        "\n",
        "\n",
        "#-----------------Logistic Regression--------------#\n",
        "#######Train the model##################\n",
        "lr = LogisticRegression(solver='lbfgs',multi_class='multinomial')\n",
        "print (\"Training the Logistic Regression model\")\n",
        "lr.fit(X_train, y_train)\n",
        "print (\"Completed LR Training\")\n",
        "\n",
        "#######Evaluate the model##################\n",
        "# they might use accuracy_score\n",
        "#This cell will take longer to run(5-10mins)!\n",
        "print (\"Predict for LR Model\")\n",
        "y_predlr = lr.predict(X_test)\n",
        "print (\"Test Accuracy(LR):\",metrics.accuracy_score(y_test, y_predlr))"
      ],
      "metadata": {
        "id": "A0I5bNQMJX4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump\n",
        "dump(dt, \"emo-detection-model.joblib\") "
      ],
      "metadata": {
        "id": "COjyfNqxJlpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finish a low accuracy model"
      ],
      "metadata": {
        "id": "mQgAa3NyJu6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract images as vectors, convert from strings to ints -- this is called a \"list comprehension\" \n",
        "x_image = np.array( [np.fromstring(df['pixels'][i], dtype=np.uint8, sep=\" \") for i in range(len(df))] ) \n",
        "\n",
        "# extract labels in the same way--through a list comprehension\n",
        "y_image = np.array( [df['emotion'][i] for i in range(len(df)) ])\n",
        "\n",
        "# generate train-test (90/10) splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_image, y_image, test_size=0.1,random_state=42)\n",
        "\n",
        "# define the model\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "\n",
        "# train \n",
        "print (\"training knn model ...\")\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# test\n",
        "print (\"Predict for KNN Model\")\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# display results\n",
        "print (\"KNN Test Accuracy on raw image inputs:\", metrics.accuracy_score(y_test, y_pred_knn))"
      ],
      "metadata": {
        "id": "iXhuwe7rJyCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}