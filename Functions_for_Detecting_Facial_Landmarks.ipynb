{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Environment"
      ],
      "metadata": {
        "id": "AdGIPywiABtE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJEAevgJ_55R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f1cda2-3bb7-4df1-9f59-2d022b64c6f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape_predictor_68_ 100%[===================>]  95.08M   131MB/s    in 0.7s    \n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# Imports the required libraries\n",
        "import cv2\n",
        "import dlib\n",
        "import math\n",
        "import unittest\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "\n",
        "from scipy.spatial import distance\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "###Getting the Dlib Shape predictor!\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/shape_predictor_68_face_landmarks.dat\"\n",
        "dlibshape_path ='./shape_predictor_68_face_landmarks.dat'\n",
        "\n",
        "print (\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pretrained Dlib model"
      ],
      "metadata": {
        "id": "vvcef0QVAXvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load's dlib's pretrained face detector model\n",
        "frontalface_detector = dlib.get_frontal_face_detector()"
      ],
      "metadata": {
        "id": "sL5JPXSCAbF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to Bounding Box Coordinates"
      ],
      "metadata": {
        "id": "OTp_Px3jAk1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rect_to_bb(rect):\n",
        "    # take a bounding predicted by dlib and convert it\n",
        "    # to the format (x, y, w, h) as we would normally do\n",
        "    # with OpenCV\n",
        "    x = rect.left()\n",
        "    y = rect.top()\n",
        "    w = rect.right() - x\n",
        "    h = rect.bottom() - y\n",
        "    return (x, y, w, h)"
      ],
      "metadata": {
        "id": "6LetYzNhAq7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detect Face in a Given Image"
      ],
      "metadata": {
        "id": "ff74CaNOAy2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_face(image_url):\n",
        "  \"\"\"\n",
        "  :type image_url: str\n",
        "  :rtype: None\n",
        "  \n",
        "  \"\"\"\n",
        "  try:\n",
        "    \n",
        "    #Decodes image address to cv2 object\n",
        "    url_response = urllib.request.urlopen(image_url)\n",
        "    img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n",
        "    image = cv2.imdecode(img_array, -1)\n",
        "    \n",
        "  except Exception as e:\n",
        "    return \"Please check the URL and try again!\"\n",
        "    \n",
        "  #Detect faces using dlib model\n",
        "  rects = frontalface_detector(image, 1)\n",
        "  \n",
        "  if len(rects) < 1:\n",
        "    return \"No Face Detected\"\n",
        "  \n",
        "  # Loop over the face detections\n",
        "  for (i, rect) in enumerate(rects):\n",
        "    # Converts dlib rectangular object to bounding box coordinates\n",
        "    (x, y, w, h) = rect_to_bb(rect)\n",
        "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "  plt.imshow(image, interpolation='nearest')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "yBp9-JIzA34n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dlib Models for Facial Landmark Detection"
      ],
      "metadata": {
        "id": "lC-KsMR6A7cV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load's dlib's pretrained face detector model\n",
        "frontalface_detector = dlib.get_frontal_face_detector()\n",
        "#Load the 68 face Landmark file\n",
        "landmark_predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')"
      ],
      "metadata": {
        "id": "3zIAgy-hBGac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return Facial Landmarks for a Given Image"
      ],
      "metadata": {
        "id": "-LSr9TXbBMi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_landmarks(image_url):\n",
        "  \"\"\"\n",
        "  :type image_url : str\n",
        "  :rtype image : cv2 object\n",
        "  :rtype landmarks : list of tuples where each tuple represents \n",
        "                     the x and y coordinates of facial keypoints\n",
        "  \"\"\"\n",
        "  \n",
        "  try:\n",
        "    \n",
        "    #Decodes image address to cv2 object\n",
        "    url_response = urllib.request.urlopen(image_url)\n",
        "    img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n",
        "    image = cv2.imdecode(img_array, -1)\n",
        "    \n",
        "  except Exception as e:\n",
        "    print (\"Please check the URL and try again!\")\n",
        "    return None,None\n",
        "  \n",
        "  #Detect the Faces within the image\n",
        "  faces = frontalface_detector(image, 1)\n",
        "  if len(faces):\n",
        "    landmarks = [(p.x, p.y) for p in landmark_predictor(image, faces[0]).parts()]\n",
        "  else:\n",
        "    return None,None\n",
        "  \n",
        "  return image,landmarks"
      ],
      "metadata": {
        "id": "C1AysH1vBPZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display Image with Facial Landmarks"
      ],
      "metadata": {
        "id": "i9t-GJisBSWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image_landmarks(image,face_landmarks):\n",
        "  \"\"\"\n",
        "  :type image_path : str\n",
        "  :type face_landmarks : list of tuples where each tuple represents \n",
        "                     the x and y coordinates of facial keypoints\n",
        "  :rtype : None\n",
        "  \"\"\"\n",
        "  radius = -1\n",
        "  circle_thickness = 5\n",
        "  image_copy = image.copy()\n",
        "  for (x, y) in face_landmarks:\n",
        "    cv2.circle(image_copy, (x, y), circle_thickness, (255,0,0), radius)\n",
        "    \n",
        "  plt.imshow(image_copy, interpolation='nearest')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "4ZPo0p38BXwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying out Facial Landmark Detection"
      ],
      "metadata": {
        "id": "ZL46I-MpBg_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image,landmarks= get_landmarks(input(\"Enter the URL of the image: \")) #url\n",
        "\n",
        "#Plot the Facial Landmarks on the face\n",
        "if landmarks:\n",
        "  plot_image_landmarks(image,landmarks)\n",
        "else:\n",
        "  print (\"No Landmarks Detected\")"
      ],
      "metadata": {
        "id": "kAYxmnlkBjn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Features"
      ],
      "metadata": {
        "id": "riKKhPu7CtMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_indices(landmarks, i_index): \n",
        "  \n",
        "  plt.scatter(x=[landmarks[i][0] for i in range(len(landmarks)//2, len(landmarks))], \n",
        "              y=[-landmarks[i][1] for i in range(len(landmarks)//2, len(landmarks))], s=50, alpha=.5, color='blue', label='second half of indices') \n",
        "\n",
        "  plt.scatter(x=[landmarks[i][0] for i in range(len(landmarks)//2)], \n",
        "              y=[-landmarks[i][1] for i in range(len(landmarks)//2)], color='red', alpha=.5, label='first half of indices')\n",
        "\n",
        "  # what should X and Y be to visualize the feature at i_index? \n",
        "  x = landmarks[i_index][0]\n",
        "  y = -landmarks[i_index][1]\n",
        "  plt.scatter(x=x, y=y, \n",
        "             color='purple', s=100, marker='x', label='feature at index %d'%i_index)\n",
        "  \n",
        "  plt.scatter(x, y, color='red', alpha=.5, label='selected indices')\n",
        "\n",
        "  plt.axis('off');\n",
        "  plt.legend(bbox_to_anchor=[1,1]);\n",
        "  plt.title('Visualizing the features we\\'ve extracted from this image',y =1.2); "
      ],
      "metadata": {
        "id": "WBmwji4wCxVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_index = 30\n",
        "show_indices(landmarks, show_index)"
      ],
      "metadata": {
        "id": "feQKG-4sEOrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(landmarks).shape"
      ],
      "metadata": {
        "id": "Wen6FEIaEPZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifying inputs to to `plot_image_landmarks` function to detect and display different parts of the face individually using Facial landmarks."
      ],
      "metadata": {
        "id": "td4TB1GzEVPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "landmark_indices = {'eyes':(36,47),\n",
        "                    \"nose\":(27,35),\n",
        "                    \"mouth\":(48,67),\n",
        "                    \"jawline\":(0,17),\n",
        "                    \"eyebrow\":(18,27)}\n",
        "\n",
        "landmark = input(\"Enter a facial landmark(eyes, nose, mouth, jawline, or eyebrow: \")\n",
        "points = np.array(landmark_indices.get(landmark))\n",
        "selected_landmarks = landmarks[points[0]:points[1]+1]\n",
        "print(selected_landmarks)\n",
        "plot_image_landmarks(image, selected_landmarks)"
      ],
      "metadata": {
        "id": "bGdPMRkiEj4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Euclidean distance"
      ],
      "metadata": {
        "id": "PAy8U9E8E0dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(p1,p2):\n",
        "  \"\"\"\n",
        "  type p1, p2 : tuple--> (x,y)\n",
        "  rtype distance: float\n",
        "  \"\"\"\n",
        "  distance =  math.sqrt((p2[0]-p1[0])**2 + (p2[1]-p1[1])**2)\n",
        "  return distance"
      ],
      "metadata": {
        "id": "nDsrXBZ2E4uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differenciating between open and close eyes between two images"
      ],
      "metadata": {
        "id": "PehIiLnxFKDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_images(image1_path, image2_path, plt_flag):\n",
        "  \"\"\"\n",
        "  type image1_path,image2_path: str\n",
        "  rtype : str\n",
        "  \"\"\"\n",
        "  \n",
        "  image1,image1_landmarks = get_landmarks(image1_path)\n",
        "  image2,image2_landmarks = get_landmarks(image2_path)\n",
        "  \n",
        "  if plt_flag:\n",
        "    #Plot image1\n",
        "    plt.imshow(image1, interpolation='nearest')\n",
        "    plt.title(\"Image1\")\n",
        "    plt.show()\n",
        "\n",
        "    #Plot image2\n",
        "    plt.imshow(image2, interpolation='nearest')\n",
        "    plt.title(\"Image2\")\n",
        "    plt.show()\n",
        "    \n",
        "  \n",
        "  # Points of interest for eyes among which distance needs to be computed\n",
        "  pairs_distance = [(37,41),(38,40),(43,47),(44,46)]\n",
        "  \n",
        "  e_sum1 = 0\n",
        "  e_sum2 = 0\n",
        "  threshold_value = 10\n",
        "  for pair in pairs_distance:\n",
        "    \n",
        "    e_sum1 = e_sum1 + euclidean_distance(image1_landmarks[pair[0]],\n",
        "                                         image1_landmarks[pair[1]])\n",
        "    e_sum2 = e_sum2 + euclidean_distance(image2_landmarks[pair[0]],\n",
        "                                         image2_landmarks[pair[1]])\n",
        "  print (e_sum1,e_sum2)\n",
        "  \n",
        "  e_difference = e_sum1 - e_sum2\n",
        "  print (e_difference)\n",
        "  if int(e_difference) == 0:\n",
        "    return (\"Both images have eyes open or closed\")\n",
        "  \n",
        "  if abs(e_difference) >= threshold_value:\n",
        "     \n",
        "    if e_difference > 0:\n",
        "        return (\"Image1 : Eyes Open, Image2 : Eyes Close\")\n",
        "    else:\n",
        "        return (\"Image1 : Eyes Close, Image2 : Eyes Open\")\n",
        "   "
      ],
      "metadata": {
        "id": "pqTLITpTFO2H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}